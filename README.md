# Awesome-Large-Model-Safety
## Safety at Scale: A Comprehensive Survey of Large Model Safety

--- 

Content:

- [Vision Foundation Model Safety](#ch2)
- [Large Language Model Safety](#ch3)
- [Vision-Language Pre-training Model Safety](#ch4)
- [Vison Language Model Safety](#ch5)
- [Diffusion Models Safety](#ch6)
- [Agent Safety](#ch7)

---
<!-- Chapter 2-->
<details>

<summary><span id="ch2">Vision Foundation Model Safety </span></summary>

##### Attacks and Defense for ViT
- Patch-fool: Are vision transformers always robust against adversarial perturbations?
    - Yonggan Fu, Shunyao Zhang, Shang Wu, Cheng Wan, Yingyan Celine Lin
    - [ICLR 2022](https://arxiv.org/pdf/2203.08392)

##### Attacks and Defense for SAM


</details>


<!-- Chapter 3-->
<details>

<summary><span id="ch3">Large Language Model Safety</summary>

##### Adversarial Attack

- Bad Characters: Imperceptible NLP Attacks
    - Nicholas Boucher, Ilia Shumailov, Ross Anderson, Nicolas Papernot
    - [S&P 2022](https://arxiv.org/abs/2106.09898)

##### Adversarial Defense

##### Jailbreak Attack

##### Jailbreak Defense

##### Prompt Injection Attacks

##### Prompt Injection Defenses

##### Backdoor Attacks

##### Backdoor Defenses

##### Safety Alignment

##### Energy Latency Attacks

##### Model Extraction Attacks

##### Data Extraction Attacks

</details>

<!-- Chapter 4-->
<details>

<summary><span id="ch4">Vision-Language Pre-training Model Safety</summary>

##### Adversarial Attacks
- Towards Adversarial Attack on Vision-Language Pre-training Models
    - Jiaming Zhang, Qi Yi, Jitao Sang
    - [S&P 2022](https://arxiv.org/abs/2106.09898)

##### Adversarial Defenses

##### Backdoor & Poisoning Attacks

##### Backdoor & Poisoning Defenses


</details>


<!-- Chapter 5-->
<details>

<summary><span id="ch5">Vison Language Model Safety</summary>

##### Adversarial Attacks

##### Jailbreak Attacks

##### Jailbreak Defenses

##### Energy Latency Attacks

##### Prompt Injection Attack

##### Backdoor & Poisoning Attacks

</details>


<!-- Chapter 6-->
<details>

<summary><span id="ch6">Diffusion Models Safety</summary>

##### Adversarial Attacks

##### Jailbreak Attacks

##### Jailbreak Defenses

##### Backdoor Attacks

##### Backdoor Defenses

##### Membership Inference Attacks

##### Data Extraction Attacks

##### Model Extraction Attacks

##### Intellectual Property Protection


</details>


<!-- Chapter 7-->
<details>

<summary><span id="ch7">Agent Safety</summary>

##### LLM Agent

- Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents.  
  - Qiusi Zhan, Zhixiang Liang, Zifan Ying, **and** Daniel Kang. *ACL*, 2024.

- BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents.  
  - Yifei Wang, Dizhan Xue, Shengjie Zhang, **and** Shengsheng Qian. *ACL*, 2024.

- TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution.  
  - Wenyue Hua, Xianjun Yang, Zelong Li, Cheng Wei, **and** Yongfeng Zhang. *ACL*, 2024.

- AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks.  
  - Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, **and** Qingyun Wu. *Neurips Workshop*, 2024.

- R-judge: Benchmarking safety risk awareness for llm agents.  
  - Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, **and** others. *EMNLP*, 2024.

- AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents.  
  - Edoardo Debenedetti, Jie Zhang, Mislav Balunovic, Luca Beurer-Kellner, Marc Fischer, **and** Florian Tram√®r. *NeurIPS*, 2024.

- Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification.  
  - Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, **and** Yang Zhang. *arXiv preprint arXiv:2407.20859*, 2024.

- AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases.  
  - Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, **and** Bo Li. *arXiv preprint arXiv:2407.12784*, 2024.

- Compromising Embodied Agents with Contextual Backdoor Attacks.  
  - Aishan Liu, Yuguang Zhou, Xianglong Liu, Tianyuan Zhang, Siyuan Liang, Jiakai Wang, Yanjun Pu, Tianlin Li, Junqi Zhang, Zhou Wenbo, **and** others. *arXiv preprint arXiv:2408.02882*, 2024.

- Psysafe: A comprehensive framework for psychological-based attack, defense, and evaluation of multi-agent system safety.  
  - Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang, Huchuan Lu, Feng Zhao, Yu Qiao, **and** Jing Shao. *arXiv preprint arXiv:2401.11880*, 2024.

- GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning.  
  - Zhen Xiang, Linzhi Zheng, Yanjie Li, Junyuan Hong, Qinbin Li, Han Xie, Jiawei Zhang, Zidi Xiong, Chulin Xie, Carl Yang, **and** others. *arXiv preprint arXiv:2406.09187*, 2024.

- SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents.  
  - Sheng Yin, Xianghe Pang, Yuanzhuo Ding, Menglan Chen, Yutong Bi, Yichen Xiong, Wenhao Huang, Zhen Xiang, Jing Shao, **and** Siheng Chen. *arXiv preprint arXiv:2412.13178*, 2024.


##### VLM Agent

- Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents.  
  - Zhan, Qiusi, Liang, Zhixiang, Ying, Zifan, **and** Kang, Daniel. *ACL*, 2024.

- Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast.  
  - Gu, Xiangming, Zheng, Xiaosen, Pang, Tianyu, Du, Chao, Liu, Qian, Wang, Ye, Jiang, Jing, **and** Lin, Min. *ICML*, 2024.

- Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification.  
  - Zhang, Boyang, Tan, Yicong, Shen, Yun, Salem, Ahmed, Backes, Michael, Zannettou, Savvas, **and** Zhang, Yang. *arXiv preprint arXiv:2407.20859*, 2024.

- Misusing tools in large language models with visual adversarial examples.  
  - Fu, Xiaohan, Wang, Zihan, Li, Shuheng, Gupta, Rajesh K, Mireshghallah, Niloofar, Berg-Kirkpatrick, Taylor, **and** Fernandes, Earlence. *arXiv preprint arXiv:2310.03185*, 2023.

- The Wolf Within: Covert Injection of Malice into MLLM Societies via an MLLM Operative.  
  - Tan, Zhen, Zhao, Chengshuai, Moraffah, Raha, Li, Yifan, Kong, Yu, Chen, Tianlong, **and** Liu, Huan. *arXiv preprint arXiv:2402.14859*, 2024.

- Adversarial Attacks on Multimodal Agents.  
  - Wu, Chen Henry, Koh, Jing Yu, Salakhutdinov, Ruslan, Fried, Daniel, **and** Raghunathan, Aditi. *arXiv preprint arXiv:2406.12814*, 2024.
    

</details>
